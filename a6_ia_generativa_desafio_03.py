# -*- coding: utf-8 -*-
"""A6_IA_GENERATIVA_DESAFIO_03

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qE4CaFfCNtpDGm0gBz_IrOZoJaljdB64

>- Fazer a integração com Langchain + OpenAI + Pinecone para
responder perguntas sobre um plano de curso de sua escolha
"""

!pip install langchain
!pip install openai
!pip install transformers
!pip install textract
!pip install tiktoken
!pip install faiss-gpu
!pip install pinecone-client
!pip install chromadb

import os
import pandas as pd
from transformers import GPT2TokenizerFast
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
from langchain.chains import ConversationalRetrievalChain
import textract

chave_openai = ""

# Leitura do arquivo CSV usando Pandas
data = pd.read_csv('/content/IA-plano-de-curso.csv')

# Extraímos o texto do PDF
doc = textract.process("/content/IA_plano_de_curso.txt")

# Criamos um arquivo txt para armazenar o texto do PDF
with open('/content/IA_plano_de_curso.txt', 'w', encoding="utf-8") as f:
    f.write(doc.decode('utf-8'))

# Escrevemos o arquivo
with open('/content/IA_plano_de_curso.txt', 'r', encoding="utf-8") as f:
    text = f.read()

# Transformamos o texto em tokens -> palavras separadas
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

def count_tokens(text: str) -> int:
    return len(tokenizer.encode(text))

# Quebramos os textos em frases / trechos
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=24,
    length_function=count_tokens,
)

# Trechos separados
chunks = text_splitter.create_documents([text])

# Transformamos as palavras e trechos em vetores
embeddings = OpenAIEmbeddings(openai_api_key=chave_openai, model="text-embedding-ada-002")

# Definimos nossa base de conhecimento
db = FAISS.from_documents(chunks, embeddings)

# Criamos nossa pergunta
query = "Em qual estado os cursos vão ocorrer?"

# Fazemos a busca por similaridade - cluster
docs = db.similarity_search(query)

# Pegamos a primeira resposta da busca semântica
docs[0]

# Definimos nosso modelo de LLM
chain = load_qa_chain(OpenAI(openai_api_key=chave_openai, temperature=0), chain_type="stuff")

# Carregamos a base de conhecimento e mandamos a pergunta
chain.run(input_documents=docs, question=query)
